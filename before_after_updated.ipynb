{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "c:\\swm\\project\\github\\swm-g17\\venv\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78055\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import pytz\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"words\")\n",
    "nltk.download(\"brown\")\n",
    "nltk.download(\"punkt\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dateutil.parser\n",
    "\n",
    "# from textblob import TextBlob\n",
    "from datetime import datetime, timedelta\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import re\n",
    "import nltk.data\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "import time\n",
    "import string\n",
    "import spacy\n",
    "\n",
    "from IPython.display import display\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "import en_core_web_lg\n",
    "\n",
    "\n",
    "# start = time.time()\n",
    "\n",
    "global words\n",
    "words = set(nltk.corpus.brown.words())\n",
    "words.add(\"aapl\")\n",
    "words.add(\"amzn\")\n",
    "words.add(\"amazon\")\n",
    "times = []\n",
    "text = []\n",
    "news_file_loc = []\n",
    "sites = []\n",
    "organizations = []\n",
    "titles = []\n",
    "published_times = []\n",
    "time_zone = pytz.timezone(\"GMT\")\n",
    "news_path = \"./data/News/\"\n",
    "json_files = glob.glob(news_path + \"*/*.json\")\n",
    "print(len(json_files))\n",
    "\n",
    "for news in json_files:\n",
    "    with open(news, \"r\", encoding=\"utf-8\") as f:\n",
    "        record = json.load(f)\n",
    "        local_time, offset = record[\"published\"].split(\"+\")\n",
    "        local_datetime = datetime.strptime(local_time, \"%Y-%m-%dT%H:%M:%S.%f\")\n",
    "        offset_datetime = datetime.strptime(offset, \"%H:%M\")\n",
    "        published_times.append(\n",
    "            local_datetime\n",
    "            - timedelta(hours=offset_datetime.hour, minutes=offset_datetime.minute)\n",
    "        )\n",
    "        iso_time = dateutil.parser.isoparse(record[\"published\"]).astimezone(time_zone)\n",
    "        times.append(iso_time)\n",
    "        news_file_loc.append(news)\n",
    "        text.append(record[\"text\"])\n",
    "        sites.append(record[\"thread\"][\"site\"])\n",
    "        titles.append(record[\"title\"])\n",
    "\n",
    "news_df = pd.DataFrame(\n",
    "    {\n",
    "        \"timestamp\": times,\n",
    "        \"published\": published_times,\n",
    "        \"text\": text,\n",
    "        \"site\": sites,\n",
    "        \"news_file_loc\": news_file_loc,\n",
    "    },\n",
    ")\n",
    "\n",
    "\n",
    "tokenizer = nltk.data.load(\"tokenizers/punkt/english.pickle\")\n",
    "\n",
    "nlp = en_core_web_lg.load()\n",
    "stopwords = nlp.Defaults.stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def filter_1sentence_only_mention(text, stock, company):\n",
    "    filterlist = []\n",
    "    sentences = tokenizer.tokenize(text)\n",
    "    if text.startswith(\"Official Notice Nr\"):\n",
    "        return None\n",
    "    for sent in sentences:\n",
    "        sent = sent.lower()\n",
    "        sent = sent.replace(\"\\n\", \" \")\n",
    "        sent = re.sub(r\"[^A-Za-z \\. \\:]+\", \"\", sent)\n",
    "        #         sent = re.sub(' +', ' ', sent)\n",
    "        #         sent = re.sub('\\.+', '.', sent)\n",
    "        if (stock in sent) or (company in sent):\n",
    "            filterlist.append(sent)\n",
    "    if len(filterlist) == 0:\n",
    "        return None\n",
    "    filtered_text = \" \".join(filterlist)\n",
    "    return filtered_text\n",
    "\n",
    "\n",
    "def filter_beforeAfter_only_mention(text, stock, company):\n",
    "    filterlist = []\n",
    "    sentences = tokenizer.tokenize(text)\n",
    "    if text.startswith(\"Official Notice Nr\"):\n",
    "        return None\n",
    "    previous_sentence = \"\"\n",
    "    next_sentence = \"\"\n",
    "    for index, sent in enumerate(sentences):\n",
    "        sent = sent.lower()\n",
    "        sent = sent.replace(\"\\n\", \" \")\n",
    "        sent = re.sub(r\"[^A-Za-z \\. \\:]+\", \"\", sent)\n",
    "        #         sent = re.sub(' +', ' ', sent)\n",
    "        #         sent = re.sub('\\.+', '.', sent)\n",
    "        next_sentence = sentences[index].lower()\n",
    "        next_sentence = next_sentence.replace(\"\\n\", \" \")\n",
    "        next_sentence = re.sub(r\"[^A-Za-z \\. \\:]+\", \"\", sent)\n",
    "        if (stock in sent) or (company in sent):\n",
    "            filterlist.append(sent)\n",
    "            filterlist.append(previous_sentence)\n",
    "            filterlist.append(next_sentence)\n",
    "        previous_sentence = sent\n",
    "    if len(filterlist) == 0:\n",
    "        return None\n",
    "    filtered_text = \" \".join(filterlist)\n",
    "    return filtered_text\n",
    "\n",
    "\n",
    "def filter_1paragraph_only_mention(text, stock, company):\n",
    "    filterlist = []\n",
    "    sentences = tokenizer.tokenize(text)\n",
    "    if text.startswith(\"Official Notice Nr\"):\n",
    "        return None\n",
    "    previous_sentence = \"\"\n",
    "    next_sentence = \"\"\n",
    "    for index, sent in enumerate(sentences):\n",
    "        sent = sent.lower()\n",
    "        sent = sent.replace(\"\\n\", \" \")\n",
    "        sent = re.sub(r\"[^A-Za-z \\. \\:]+\", \"\", sent)\n",
    "        #         sent = re.sub(' +', ' ', sent)\n",
    "        #         sent = re.sub('\\.+', '.', sent)\n",
    "        if (stock in sent) or (company in sent):\n",
    "            filterlist.append(sent)\n",
    "        previous_sentence = sent\n",
    "    if len(filterlist) == 0:\n",
    "        return None\n",
    "    filtered_text = \" \".join(filterlist)\n",
    "    return filtered_text\n",
    "\n",
    "\n",
    "def filterStopwords(text):\n",
    "    return \" \".join([w for w in text.split() if w not in stopwords])\n",
    "\n",
    "\n",
    "def lemmatize(text):\n",
    "    return \" \".join([lemma.lemma_ if lemma.lemma_ else text for lemma in nlp(text)])\n",
    "\n",
    "\n",
    "def filter_only_language(mention_filtered_text):\n",
    "    mention_filtered_text = mention_filtered_text.translate(\n",
    "        str.maketrans(\"\", \"\", string.punctuation)\n",
    "    )\n",
    "    # mention_filtered_text = mention_filtered_text.translate(\n",
    "    #     str.maketrans(\"\", \"\", string.digits)\n",
    "    # )\n",
    "    mention_filtered_list = word_tokenize(str(mention_filtered_text))\n",
    "    global words\n",
    "    preparse_length = float(len(mention_filtered_list))\n",
    "    if preparse_length:\n",
    "        parsed_filter_list = [w for w in mention_filtered_list if w in words]\n",
    "        parsed_length = float(len(parsed_filter_list))\n",
    "        # 80%, min=5 -> ~1300 records for amazon5_df : 400s\n",
    "        # 50%, min=5 -> ~4800 records ...            : 900s\n",
    "        # 50%, min=10 -> ~4300 records ...           : 900s\n",
    "        # 25%, min=10 -> ~4500 records ...           : 950s\n",
    "        if parsed_length / preparse_length < 0.5 or parsed_length < 10:\n",
    "            mention_filtered_text = None\n",
    "        else:\n",
    "            mention_filtered_text = \" \".join(parsed_filter_list)\n",
    "    else:\n",
    "        mention_filtered_text = None\n",
    "    return mention_filtered_text"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78055\n",
      "76023\n",
      "before last drop 17361\n",
      "after last drop 17361\n"
     ]
    }
   ],
   "source": [
    "print(len(news_df))\n",
    "news_df.drop_duplicates(subset=[\"timestamp\", \"published\", \"text\", \"site\"], inplace=True)\n",
    "print(len(news_df))\n",
    "news_df.to_csv(\"data/news_df.csv\", encoding=\"utf-8\", index=False)\n",
    "\n",
    "amazon_df = news_df.copy(deep=True)\n",
    "apple_df = news_df.copy(deep=True)\n",
    "\n",
    "amazon_df[\"filteredtext\"] = amazon_df[\"text\"].apply(\n",
    "    filter_beforeAfter_only_mention, args=(\"amzn\", \"amazon\")\n",
    "    # filter_1sentence_only_mention, args=(\"amzn\", \"amazon\")\n",
    ")\n",
    "amazon_df.dropna(subset=[\"filteredtext\"], inplace=True)\n",
    "amazon_df[\"filteredtext\"] = amazon_df[\"filteredtext\"].apply(filterStopwords)\n",
    "amazon_df.dropna(subset=[\"filteredtext\"], inplace=True)\n",
    "amazon_df[\"filteredtext\"] = amazon_df[\"filteredtext\"].apply(filter_only_language)\n",
    "amazon_df.dropna(subset=[\"filteredtext\"], inplace=True)\n",
    "amazon_df[\"filteredtext\"] = amazon_df[\"filteredtext\"].apply(lemmatize)\n",
    "print(\"before last drop\", len(amazon_df))\n",
    "amazon_df.dropna(subset=[\"filteredtext\"], inplace=True)\n",
    "print(\"after last drop\", len(amazon_df))\n",
    "\n",
    "\n",
    "apple_df[\"filteredtext\"] = apple_df[\"text\"].apply(\n",
    "    filter_beforeAfter_only_mention, args=(\"aapl\", \"apple\")\n",
    "    # filter_1sentence_only_mention, args=(\"aapl\", \"apple\")\n",
    ")\n",
    "apple_df.dropna(subset=[\"filteredtext\"], inplace=True)\n",
    "apple_df[\"filteredtext\"] = apple_df[\"filteredtext\"].apply(filterStopwords)\n",
    "apple_df.dropna(subset=[\"filteredtext\"], inplace=True)\n",
    "apple_df[\"filteredtext\"] = apple_df[\"filteredtext\"].apply(filter_only_language)\n",
    "apple_df.dropna(subset=[\"filteredtext\"], inplace=True)\n",
    "apple_df[\"filteredtext\"] = apple_df[\"filteredtext\"].apply(lemmatize)\n",
    "apple_df.dropna(subset=[\"filteredtext\"], inplace=True)\n",
    "\n",
    "# Just for safety :P\n",
    "amazon5_df = amazon_df.copy(deep=True)\n",
    "amazon15_df = amazon_df.copy(deep=True)\n",
    "amazon30_df = amazon_df.copy(deep=True)\n",
    "amazon60_df = amazon_df.copy(deep=True)\n",
    "amazon240_df = amazon_df.copy(deep=True)\n",
    "amazon1440_df = amazon_df.copy(deep=True)\n",
    "\n",
    "\n",
    "apple5_df = apple_df.copy(deep=True)\n",
    "apple15_df = apple_df.copy(deep=True)\n",
    "apple30_df = apple_df.copy(deep=True)\n",
    "apple60_df = apple_df.copy(deep=True)\n",
    "apple240_df = apple_df.copy(deep=True)\n",
    "apple1440_df = apple_df.copy(deep=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "amazon5_df[\"rounded_time\"] = (amazon5_df[\"published\"]).dt.round(\"5min\")\n",
    "amazon15_df[\"rounded_time\"] = (amazon15_df[\"published\"]).dt.round(\"15min\")\n",
    "amazon30_df[\"rounded_time\"] = (amazon30_df[\"published\"]).dt.round(\"30min\")\n",
    "amazon60_df[\"rounded_time\"] = (amazon60_df[\"published\"]).dt.round(\"1H\")\n",
    "amazon240_df[\"rounded_time\"] = (amazon240_df[\"published\"]).dt.round(\"4H\")\n",
    "amazon1440_df[\"rounded_time\"] = (amazon1440_df[\"published\"]).dt.round(\"1D\")\n",
    "\n",
    "apple5_df[\"rounded_time\"] = (apple5_df[\"published\"]).dt.round(\"5min\")\n",
    "apple15_df[\"rounded_time\"] = (apple15_df[\"published\"]).dt.round(\"15min\")\n",
    "apple30_df[\"rounded_time\"] = (apple30_df[\"published\"]).dt.round(\"30min\")\n",
    "apple60_df[\"rounded_time\"] = (apple60_df[\"published\"]).dt.round(\"1H\")\n",
    "apple240_df[\"rounded_time\"] = (apple240_df[\"published\"]).dt.round(\"4H\")\n",
    "apple1440_df[\"rounded_time\"] = (apple1440_df[\"published\"]).dt.round(\"1D\")\n",
    "\n",
    "CHARTS_dir = \"./data/CHARTS/\"\n",
    "\n",
    "\n",
    "def join_CHARTS_data_and_GT(source_df, interval, company):\n",
    "    CHARTS_df = pd.read_csv(\n",
    "        os.path.join(CHARTS_dir, company + str(interval) + \".csv\"),\n",
    "        header=None,\n",
    "        index_col=False,\n",
    "    )\n",
    "    CHARTS_df.columns = [\n",
    "        \"year.month.day\",\n",
    "        \"24hr\",\n",
    "        \"Open\",\n",
    "        \"High\",\n",
    "        \"Low\",\n",
    "        \"Close\",\n",
    "        \"Volume\",\n",
    "    ]\n",
    "    CHARTS_df[\"temp_timestamp\"] = pd.to_datetime(\n",
    "        CHARTS_df[\"year.month.day\"].astype(str) + \"T\" + CHARTS_df[\"24hr\"],\n",
    "        format=\"%Y.%m.%dT%H:%M\",\n",
    "    )\n",
    "\n",
    "    CHARTS_df[\"label\"] = (CHARTS_df[\"Close\"] - CHARTS_df[\"Open\"]).apply(\n",
    "        lambda x: 1 if x > 0 else -1\n",
    "    )\n",
    "\n",
    "    right_on = \"rounded_time\"\n",
    "    if interval == 60:\n",
    "        CHARTS_df[\"timestamp\"] = CHARTS_df[\"temp_timestamp\"].dt.round(\"1H\")\n",
    "    elif interval == 240:\n",
    "        CHARTS_df[\"timestamp\"] = CHARTS_df[\"temp_timestamp\"].dt.round(\"4H\")\n",
    "    else:\n",
    "        CHARTS_df[\"timestamp\"] = CHARTS_df[\"temp_timestamp\"]\n",
    "\n",
    "    joined_df = pd.merge(\n",
    "        CHARTS_df,\n",
    "        source_df,\n",
    "        left_on=\"timestamp\",\n",
    "        right_on=right_on,\n",
    "        how=\"left\",\n",
    "        suffixes=(\"\", \"_y\"),\n",
    "    )\n",
    "    joined_df.drop(joined_df.filter(regex=\"_y$\").columns.tolist(), axis=1, inplace=True)\n",
    "    joined_df.drop([\"temp_timestamp\"], axis=1, inplace=True)\n",
    "\n",
    "    ########### READ AFTER TESTING\n",
    "    # joined_df.drop([\"entities\"], axis=1, inplace=True)\n",
    "    return joined_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "amazon5_df = join_CHARTS_data_and_GT(amazon5_df, 5, \"AMAZON\")\n",
    "amazon15_df = join_CHARTS_data_and_GT(amazon15_df, 15, \"AMAZON\")\n",
    "amazon30_df = join_CHARTS_data_and_GT(amazon30_df, 30, \"AMAZON\")\n",
    "amazon60_df = join_CHARTS_data_and_GT(amazon60_df, 60, \"AMAZON\")\n",
    "amazon240_df = join_CHARTS_data_and_GT(amazon240_df, 240, \"AMAZON\")\n",
    "amazon1440_df = join_CHARTS_data_and_GT(amazon1440_df, 1440, \"AMAZON\")\n",
    "\n",
    "apple5_df = join_CHARTS_data_and_GT(apple5_df, 5, \"APPLE\")\n",
    "apple15_df = join_CHARTS_data_and_GT(apple15_df, 15, \"APPLE\")\n",
    "apple30_df = join_CHARTS_data_and_GT(apple30_df, 30, \"APPLE\")\n",
    "apple60_df = join_CHARTS_data_and_GT(apple60_df, 60, \"APPLE\")\n",
    "apple240_df = join_CHARTS_data_and_GT(apple240_df, 240, \"APPLE\")\n",
    "apple1440_df = join_CHARTS_data_and_GT(apple1440_df, 1440, \"APPLE\")\n",
    "\n",
    "amazon5_df.dropna(inplace=True)\n",
    "amazon15_df.dropna(inplace=True)\n",
    "amazon30_df.dropna(inplace=True)\n",
    "amazon60_df.dropna(inplace=True)\n",
    "amazon240_df.dropna(inplace=True)\n",
    "amazon1440_df.dropna(inplace=True)\n",
    "\n",
    "apple5_df.dropna(inplace=True)\n",
    "apple15_df.dropna(inplace=True)\n",
    "apple30_df.dropna(inplace=True)\n",
    "apple60_df.dropna(inplace=True)\n",
    "apple240_df.dropna(inplace=True)\n",
    "apple1440_df.dropna(inplace=True)\n",
    "\n",
    "amazon5_df.drop([\"text\"], axis=1, inplace=True)\n",
    "amazon15_df.drop([\"text\"], axis=1, inplace=True)\n",
    "amazon30_df.drop([\"text\"], axis=1, inplace=True)\n",
    "amazon60_df.drop([\"text\"], axis=1, inplace=True)\n",
    "amazon240_df.drop([\"text\"], axis=1, inplace=True)\n",
    "amazon1440_df.drop([\"text\"], axis=1, inplace=True)\n",
    "\n",
    "apple5_df.drop([\"text\"], axis=1, inplace=True)\n",
    "apple15_df.drop([\"text\"], axis=1, inplace=True)\n",
    "apple30_df.drop([\"text\"], axis=1, inplace=True)\n",
    "apple60_df.drop([\"text\"], axis=1, inplace=True)\n",
    "apple240_df.drop([\"text\"], axis=1, inplace=True)\n",
    "apple1440_df.drop([\"text\"], axis=1, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def remove_duplicates(df):\n",
    "    df.sort_values(by=[\"filteredtext\"], inplace=True)\n",
    "    chunk_size = 1000\n",
    "    chunks = []\n",
    "    count = 0\n",
    "    # come back and take care of boundary conditions\n",
    "    while not df.empty:\n",
    "        # print(len(df))\n",
    "        count += 1\n",
    "        # print(\"iteration\", count)\n",
    "        chunks.append(df[:chunk_size])\n",
    "        df = df.iloc[chunk_size - 1 :]\n",
    "        if count != 1:\n",
    "            # print(str(chunks[-1].iloc[-1][\"filteredtext\"]))\n",
    "            # print(str(chunks[-2].iloc[0][\"filteredtext\"]))\n",
    "            if str(chunks[-1].iloc[-1][\"filteredtext\"]) == str(\n",
    "                chunks[-2].iloc[0][\"filteredtext\"]\n",
    "            ):\n",
    "                chunks[-2].drop(chunks[-2].tail(1).index, inplace=True)\n",
    "\n",
    "    for chunk in chunks:\n",
    "        chunk[\"row_string\"] = (\n",
    "            chunk[\"published\"].astype(str)\n",
    "            + chunk[\"site\"].astype(str)\n",
    "            + chunk[\"filteredtext\"].astype(str)\n",
    "        )\n",
    "\n",
    "        chunk.drop_duplicates(subset=[\"row_string\"], keep=\"first\", inplace=True)\n",
    "    df = pd.concat(chunks, ignore_index=True)\n",
    "    df.drop([\"row_string\"], axis=1, inplace=True)\n",
    "    # print(len(df))\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-531e5dfb380e>:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  chunk[\"row_string\"] = (\n",
      "<ipython-input-6-531e5dfb380e>:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  chunk.drop_duplicates(subset=[\"row_string\"], keep=\"first\", inplace=True)\n"
     ]
    }
   ],
   "source": [
    "amazon5_df = remove_duplicates(amazon5_df)\n",
    "amazon15_df = remove_duplicates(amazon15_df)\n",
    "amazon30_df = remove_duplicates(amazon30_df)\n",
    "amazon60_df = remove_duplicates(amazon60_df)\n",
    "amazon240_df = remove_duplicates(amazon240_df)\n",
    "amazon1440_df = remove_duplicates(amazon1440_df)\n",
    "\n",
    "apple5_df = remove_duplicates(apple5_df)\n",
    "apple15_df = remove_duplicates(apple15_df)\n",
    "apple30_df = remove_duplicates(apple30_df)\n",
    "apple60_df = remove_duplicates(apple60_df)\n",
    "apple240_df = remove_duplicates(apple240_df)\n",
    "apple1440_df = remove_duplicates(apple1440_df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "amazon5_df.to_csv(\"data/amazon5.csv\", encoding=\"utf-8\", index=False)\n",
    "amazon15_df.to_csv(\"data/amazon15.csv\", encoding=\"utf-8\", index=False)\n",
    "amazon30_df.to_csv(\"data/amazon30.csv\", encoding=\"utf-8\", index=False)\n",
    "amazon60_df.to_csv(\"data/amazon60.csv\", encoding=\"utf-8\", index=False)\n",
    "amazon240_df.to_csv(\"data/amazon240.csv\", encoding=\"utf-8\", index=False)\n",
    "amazon1440_df.to_csv(\"data/amazon1440.csv\", encoding=\"utf-8\", index=False)\n",
    "\n",
    "apple5_df.to_csv(\"data/apple5.csv\", encoding=\"utf-8\", index=False)\n",
    "apple15_df.to_csv(\"data/apple15.csv\", encoding=\"utf-8\", index=False)\n",
    "apple30_df.to_csv(\"data/apple30.csv\", encoding=\"utf-8\", index=False)\n",
    "apple60_df.to_csv(\"data/apple60.csv\", encoding=\"utf-8\", index=False)\n",
    "apple240_df.to_csv(\"data/apple240.csv\", encoding=\"utf-8\", index=False)\n",
    "apple1440_df.to_csv(\"data/apple1440.csv\", encoding=\"utf-8\", index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# end = time.time()\n",
    "# print(end - start)\n",
    "\n",
    "\n",
    "# import pysentiment2 as ps\n",
    "\n",
    "\n",
    "# def get_glove_embeddings(df):\n",
    "#     word_list = []\n",
    "#     for i in df[\"filteredtext\"]:\n",
    "#         x = i[1:-1].split(\", \")\n",
    "#         words = []\n",
    "#         for j in x:\n",
    "#             s = j.split(\" \")\n",
    "#             for k in s:\n",
    "#                 words.append(k)\n",
    "#         word_list.append(words)\n",
    "#     filename = \"./data/glove.6B.100d.txt.word2vec\"\n",
    "#     model = KeyedVectors.load_word2vec_format(filename, binary=False)\n",
    "\n",
    "#     embedding_list = []\n",
    "#     for i in word_list:\n",
    "#         embeddings = []\n",
    "#         for j in i:\n",
    "#             try:\n",
    "#                 glov = model[j]\n",
    "#                 embeddings.append(glov)\n",
    "#             except:\n",
    "#                 continue\n",
    "#         embedding_list.append(embeddings)\n",
    "#     return embedding_list\n",
    "\n",
    "\n",
    "# def calculate_polarity_subjectivity(df):\n",
    "#     lm = ps.LM()\n",
    "#     hiv4 = ps.HIV4()\n",
    "#     polarity_array = []\n",
    "#     subjectivity_array = []\n",
    "\n",
    "#     hiv_polarity = []\n",
    "#     hiv_subjectivity = []\n",
    "#     count = 0\n",
    "#     count1 = 0\n",
    "#     for x in range(len(df[\"filteredtext\"])):\n",
    "#         tokens_m = lm.tokenize(df[\"filteredtext\"][x])\n",
    "#         score_m = lm.get_score(tokens_m)\n",
    "#         polarity_array.append(score_m[\"Polarity\"])\n",
    "#         subjectivity_array.append(score_m[\"Subjectivity\"])\n",
    "\n",
    "#         tokens_hiv = hiv4.tokenize(df[\"filteredtext\"][x])\n",
    "#         score_hiv = hiv4.get_score(tokens_hiv)\n",
    "\n",
    "#         hiv_polarity.append(score_hiv[\"Polarity\"])\n",
    "#         hiv_subjectivity.append(score_hiv[\"Subjectivity\"])\n",
    "#         if score_m[\"Polarity\"] * score_hiv[\"Polarity\"] < 0:\n",
    "#             count += 1\n",
    "\n",
    "#     feature_df = pd.DataFrame()\n",
    "#     feature_df[\"Mcdonald_Polarity\"] = polarity_array\n",
    "#     feature_df[\"Mcdonald_Subjectivity\"] = subjectivity_array\n",
    "\n",
    "#     feature_df[\"HIV_Polarity\"] = hiv_polarity\n",
    "#     feature_df[\"HIV_Subjectivity\"] = hiv_subjectivity\n",
    "\n",
    "#     feature_df[\"GloVe_embedding\"] = get_glove_embeddings(df)\n",
    "\n",
    "#     return feature_df\n",
    "\n",
    "\n",
    "# feature_amazon5 = calculate_polarity_subjectivity(amazon5_df)\n",
    "# feature_amazon15 = calculate_polarity_subjectivity(amazon15_df)\n",
    "# feature_amazon30 = calculate_polarity_subjectivity(amazon30_df)\n",
    "# feature_amazon60 = calculate_polarity_subjectivity(amazon60_df)\n",
    "# feature_amazon240 = calculate_polarity_subjectivity(amazon240_df)\n",
    "# feature_amazon1440 = calculate_polarity_subjectivity(amazon1440_df)\n",
    "\n",
    "# feature_apple5 = calculate_polarity_subjectivity(apple5_df)\n",
    "# feature_apple15 = calculate_polarity_subjectivity(apple15_df)\n",
    "# feature_apple30 = calculate_polarity_subjectivity(apple30_df)\n",
    "# feature_apple60 = calculate_polarity_subjectivity(apple60_df)\n",
    "# feature_apple240 = calculate_polarity_subjectivity(apple240_df)\n",
    "# feature_apple1440 = calculate_polarity_subjectivity(apple1440_df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}