# -*- coding: utf-8 -*-
"""Prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1q5xUUtbtM_idzj7r-y5wAGfMVbJGVdH0
"""

import nltk
nltk.downloader.download('vader_lexicon')
# from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import LancasterStemmer,PorterStemmer
from nltk.sentiment.vader import SentimentIntensityAnalyzer
from tqdm import tqdm
from sklearn.feature_extraction.text import CountVectorizer
nltk.download('stopwords')
nltk.download('punkt')
stopwords = nltk.corpus.stopwords.words('english')
from nltk.stem import LancasterStemmer,PorterStemmer
import unicodedata
import re
import pandas as pd
import numpy as np

def removenonAscii(words):
    words_list=[]
    for w in words:
        w=re.sub('[^a-zA-Z]+','',re.sub(r'[\W\d]','',w.lower()))
        format_words=unicodedata.normalize('NFKD', w).encode('ascii', 'ignore').decode('utf-8', 'ignore')
        words_list.append(format_words)
        
    return words_list
def stemmatize(words):
    stemmer = PorterStemmer()
    words_list=[]
    for word in words:
        word=stemmer.stem(word)
        if word not in words_list:
            words_list.append(word)
    return words_list

def filterStopWords(words):
    words_list=[]
    for w in words:
        # print(w)
        # print(stopwords)
        if w not in stopwords:
            words_list.append(w)
    return words_list

def filterLinks(words):
    words_list=[]
    for w in words:
        if not re.match('[www]',w):
            words_list.append(w)
    return words_list

def removeSpace(words):
    words_list=[]
    for w in words:
        if w!='':
            words_list.append(w)
    return words_list


def dataPreprocessing(sentence):
    sentence=removenonAscii(sentence)
    sentence=removeSpace(sentence)
    sentence=filterStopWords(sentence)
    sentence=stemmatize(sentence)
    sentence=filterLinks(sentence)
    return sentence


def get_sentences(news_article):
    paragraphs = nltk.sent_tokenize(news_article.lower())
    sentences=[]
    filter_words=['amazon','apple','aapl','amzn']
    amazon_keywords = ['amazon', 'amzn']
    apple_keywords = ['apple', 'aapl']

    for para in paragraphs:
      sentences_list = para.split("\n")
      for sentence in sentences_list:
        if any(key in sentence for key in amazon_keywords):
          sentences.append(sentence)
        if any(key in sentence for key in apple_keywords):
          sentences.append(sentence)


    if len(sentences) == 0:
      return None
    
    processed_data_df = pd.DataFrame({'text':sentences})

    processed_data_df=processed_data_df[processed_data_df['text'].str.match('^[A-Z a-z 0-9]+')]

    processed_data_df=processed_data_df.drop_duplicates(keep=False).reset_index(drop=True)

    processed_data_df['words'] = processed_data_df.text.apply(word_tokenize)
    processed_data_df['words'] = processed_data_df.words.apply(dataPreprocessing)
    processed_data_df['text'] = processed_data_df.words.apply(lambda words: " ".join(words))
    processed_data_df = processed_data_df.drop(columns="words")
    return processed_data_df

import pickle
def SentimentVectorizer():
  path = "./pickles/vectorizer"
  infile = open(path,'rb')
  vectorizer_file = pickle.load(infile)
  infile.close()

  vectorizer = vectorizer_file['vectorizer']
  return vectorizer

def extractFeatures(Xtest):
  sentiment = SentimentIntensityAnalyzer()
  Postive_test_sentiment = []
  Negative_test_sentiment = []
  vectorizer = SentimentVectorizer()

  for i in tqdm(Xtest):
      negative= sentiment.polarity_scores(i)["neg"]
      positive= sentiment.polarity_scores(i)["pos"]
      Postive_test_sentiment.append(positive)
      Negative_test_sentiment.append(negative)
  
  Xtest_2gram_features = vectorizer.transform(Xtest)
  Sentiment_X_test = np.column_stack((Postive_test_sentiment, Negative_test_sentiment))

  Xtest = np.concatenate((Sentiment_X_test,Xtest_2gram_features.toarray()), axis = 1)
  return Xtest

def LRPrediction(text):
  if text.lower() == 'apple':
    path = "./pickles/Apple_Logistic_Regression.sav"
  else:
    path = "./pickles/Amazon_Logistic_Regression.sav"

  
  infile = open(path,'rb')
  LR = pickle.load(infile)
  infile.close() 

  return LR

def MLPPrediction(text):
  if text.lower() == 'apple':
    path = "./pickles/Apple_MLP.sav"
  else:
    path = "./pickles/Amazon_MLP.sav"
  infile = open(path,'rb')
  MLP = pickle.load(infile)
  infile.close()

  return MLP

def LinearSVMPrediction(text):
  if text.lower() == 'apple':
    path = "./pickles/Apple_SVM_LinearSVC.sav"
  else:
    path = "./pickles/Amazon_SVM_LinearSVC.sav"
  
  infile = open(path,'rb')
  LinearSVC = pickle.load(infile)
  infile.close()

  return LinearSVC

def NaiveBayesPrediction(text):
  if text.lower() == 'apple':
    path = "./pickles/Apple_Naive_Bayes.sav"
  else:
    path = "./pickles/Amazon_Naive_Bayes.sav"
  
  infile = open(path,'rb')
  NB = pickle.load(infile)
  infile.close()

  return NB

def RandomForestPrediction(text):
  if text.lower() == 'apple':
    path = "./pickles/Apple_Naive_Bayes.sav"
  else:
    path = "./pickles/Amazon_Naive_Bayes.sav"
  
  infile = open(path,'rb')
  NB = pickle.load(infile)
  infile.close()

  return NB
  # if text.lower() == 'apple':
  #   path = "./pickles/RF_Apple.sav"
  # else:
  #   path = "./pickles/RF_Amazon.sav"
  
  # infile = open(path,'rb')
  # RF = pickle.load(infile)
  # infile.close()

  # return RF

def preprocessingFeatureExtraction(text):
  sentences = get_sentences(text)
  sentences_test = extractFeatures(sentences)
  return sentences_test

def predictStockPrices(text,model,newsType):
  # print("&&&&&&&&&&&&&&&&&&&&")
  sentences_test = preprocessingFeatureExtraction(text)
  # print(sentences_test)
  if(len(sentences_test)!=0 and newsType == 'Apple'):
    if model.lower() == 'logistic regression':
      model = LRPrediction('Apple')
    elif model.lower() == 'naive bayes':
      model = NaiveBayesPrediction('Apple')
    elif model.lower() =='linear svm':
      model = LinearSVMPrediction('Apple')
    elif model.lower() == 'mlp':
      model = MLPPrediction('Apple')
    else:
      model = RandomForestPrediction('Apple')

    prediction_apple = model.predict(sentences_test)
    if(prediction_apple[0] == 0):
      return False
    else:
      return True

  if(len(sentences_test)!=0 and newsType == 'Amazon'):
    if model.lower() == 'logistic regression':
      # print(model)
      model = LRPrediction('Amazon')
    elif model.lower() == 'naive bayes':
      model = NaiveBayesPrediction('Amazon')
    elif model.lower() =='linear svm':
      model = LinearSVMPrediction('Amazon')
    elif model.lower() == 'mlp':
      model = MLPPrediction('Amazon')
    else:
      model = RandomForestPrediction('Amazon')

    prediction_amazon = model.predict(sentences_test)
    # print(sentences_test)
    # print("&*&&*((()())(P*)*&*)&")
    # print(prediction_amazon)
    if(prediction_amazon[0] == 0):
      return False
    else:
      return True


def predictAllModels(text,newsType):
  sentences_test = preprocessingFeatureExtraction(text)
  model = LRPrediction(newsType)
  model_LR  = model.predict(sentences_test)
  model = NaiveBayesPrediction(newsType)
  model_NB = model.predict(sentences_test)
  model = LinearSVMPrediction(newsType)
  model_LSVM = model.predict(sentences_test)
  model = MLPPrediction(newsType)
  model_MLP = model.predict(sentences_test)
  result_LSTM=glove_feature_lstm(text,newsType)

  data = {}
  data['Logistic Regression'] = True if model_LR[0] else False
  data['Naive Bayes'] = True if model_NB[0] else False
  data['Linear SVM'] = True if model_LSVM[0] else False
  data['MLP'] = True if model_MLP[0] else False
  data['LSTM']=result_LSTM

  json_data = json.dumps(data)

  return json_data

import json
def prediction(text,model,newsType):
  if model != 'All':
    data={}
    # print("****(*******")
    # print(text)
    # print("****(*******")
    if model != 'LSTM':
      result = predictStockPrices(text,model,newsType)
    else:
      result=glove_feature_lstm(text,newsType)
    data[model] = result
    json_data = json.dumps(data)
    return json_data
  else:
    return predictAllModels(text,newsType)

#Example
# text = "aapl account troubling development aapl account value standard widely consider good representation stock market broad economy"
# result = prediction(text,'All','Apple')
# print(result)


import pandas as pd
import numpy as np
import os
from tqdm import tqdm_notebook
from tensorflow.keras import regularizers, initializers, optimizers, callbacks
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.preprocessing.text import Tokenizer
from keras.utils.np_utils import to_categorical
from tensorflow.keras.layers import *
from tensorflow.keras.models import Model

import pickle
from sklearn.metrics import classification_report

import pandas as pd
import numpy as np
import os

from IPython.display import display
from gensim.scripts.glove2word2vec import glove2word2vec
from gensim.models import KeyedVectors
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import PCA
from sklearn.preprocessing import normalize
from sklearn.feature_extraction.text import TfidfVectorizer
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report
from sklearn.metrics import f1_score
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score
from sklearn.neural_network import MLPClassifier
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import LinearSVC
from sklearn.feature_extraction.text import CountVectorizer
from scipy.sparse import hstack
import nltk
nltk.downloader.download('vader_lexicon')
from nltk.sentiment.vader import SentimentIntensityAnalyzer
from tqdm import tqdm
import scipy as sp
import pickle
from sklearn.model_selection import train_test_split
import pysentiment2 as ps 
import pandas as pd
import numpy as np
import os
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import PCA
from sklearn.preprocessing import normalize
from sklearn.feature_extraction.text import TfidfVectorizer
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report
from sklearn.metrics import f1_score
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score
from sklearn.neural_network import MLPClassifier
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report
from tensorflow import keras


def glove_feature_lstm(text,company):
  text1 = get_sentences(text)
  # print(text1)
  # print("859804803083")
  text=""
  for st in text1:
    text=text+" "+st
  text=text[1:]
  # print(text)
  MAX_NB_WORDS = 100000    # max no. of words for tokenizer
  MAX_SEQUENCE_LENGTH = 2000 # max length of each entry (sentence), including padding
  VALIDATION_SPLIT = 0.2   # data for validation (not used in training)
  EMBEDDING_DIM = 100      # embedding dimensions for word vectors (word2vec/GloVe)
  GLOVE_DIR = "SWM_Data/glove.6B."+str(EMBEDDING_DIM)+"d.txt"
  x_val=[text]
  tokenizer = Tokenizer(num_words=MAX_NB_WORDS)
  tokenizer.fit_on_texts(x_val)
  sequences = tokenizer.texts_to_sequences(x_val)
  word_index = tokenizer.word_index
  # print('Vocabulary size:', len(word_index))
  data = pad_sequences(sequences, padding = 'post', maxlen = MAX_SEQUENCE_LENGTH)
  recon_model=None
  if company == 'Amazon':
    recon_model = keras.models.load_model('pickles/amazon_glov_lstm')
  else:
    recon_model = keras.models.load_model('pickles/apple_glov_lstm')
  y_pred = recon_model.predict(data, batch_size=64, verbose=0)
  # y_pred_bool = np.argmax(y_pred, axis=1)
  # print(y_pred)
  # print("o543u50394u5")
  if(y_pred[0] >= 0.5):
      return True
  else:
      return False

